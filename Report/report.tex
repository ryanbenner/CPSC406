\documentclass{article}

\usepackage{tikz} 
\usetikzlibrary{automata, positioning, arrows} 

\usepackage{amsthm}
\usepackage{amsfonts}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{fullpage}
\usepackage{color}
\usepackage{parskip}
\usepackage{hyperref}
  \hypersetup{
    colorlinks = true,
    urlcolor = blue,       % color of external links using \href
    linkcolor= blue,       % color of internal links 
    citecolor= blue,       % color of links to bibliography
    filecolor= blue,        % color of file links
    }
    
\usepackage{listings}
\usepackage[utf8]{inputenc}                                                    
\usepackage[T1]{fontenc}      
\usepackage{pdfpages}                                                 

\definecolor{dkgreen}{rgb}{0,0.6,0}
\definecolor{gray}{rgb}{0.5,0.5,0.5}
\definecolor{mauve}{rgb}{0.58,0,0.82}

\lstset{frame=tb,
  language=haskell,
  aboveskip=3mm,
  belowskip=3mm,
  showstringspaces=false,
  columns=flexible,
  basicstyle={\small\ttfamily},
  numbers=none,
  numberstyle=\tiny\color{gray},
  keywordstyle=\color{blue},
  commentstyle=\color{dkgreen},
  stringstyle=\color{mauve},
  breaklines=true,
  breakatwhitespace=true,
  tabsize=3
}

\newtheoremstyle{theorem}
  {\topsep}   % ABOVESPACE
  {\topsep}   % BELOWSPACE
  {\itshape\/}  % BODYFONT
  {0pt}       % INDENT (empty value is the same as 0pt)
  {\bfseries} % HEADFONT
  {.}         % HEADPUNCT
  {5pt plus 1pt minus 1pt} % HEADSPACE
  {}          % CUSTOM-HEAD-SPEC
\theoremstyle{theorem} 
   \newtheorem{theorem}{Theorem}[section]
   \newtheorem{corollary}[theorem]{Corollary}
   \newtheorem{lemma}[theorem]{Lemma}
   \newtheorem{proposition}[theorem]{Proposition}
\theoremstyle{definition}
   \newtheorem{definition}[theorem]{Definition}
   \newtheorem{example}[theorem]{Example}
\theoremstyle{remark}    
  \newtheorem{remark}[theorem]{Remark}

\title{CPSC-406 Report}
\author{Ryan Benner  \\ Chapman University}

\date{\today} 

\begin{document}

\maketitle

\begin{abstract}
  This report is an in depth analysis and exploration of the foundational topics surrounding computational theory and algorithm analysis that were covered throughout the semester long course. These topics include things such as deterministic and non deterministic finite automata, regular expressions, formal languages, complexity analysis, Turing Machines, and decidability. The report explores the Halting problem in particular depth, and discusses the practical implications of the Halting problem and various other theories. It discusses the real life impacts of these theories on the practical field of software engineering, particularly focusing on both the challenges as well as the strategies associated with algorithm efficiency, computational limits, and how these manifest in real world applications. Finally, there is ample reflection upon the importance of these concepts, their relevance to modern day software development practices, as well as potential improvements upon how practices may evolve in the future due to rapid advancements in technology.
\end{abstract}

\setcounter{tocdepth}{3}
\tableofcontents

\section{Introduction}\label{intro}

This report takes an in depth look at the main topics covered throughout the algorithm analysis course, and connects them to real-world software engineering practices. Throughout the semester, we worked with deterministic and non-deterministic finite automata, gaining an understanding of how they influence decision making in tasks like parsing and language recognition. Next, we looked at regular expressions and formal language theory, which provided methods for recognizing patterns in data and building complex systems such as compilers. As the course progressed, more advanced topics were explored, such as Turing machines, complexity analysis, and theoretical limits of computation, with the Halting Problem being a key example of an undecidable problem.

One of the most important takeaways from the course was the realization that not everything is solvable, even in theory. Learning about the Halting Problem made it very clear to me that there are boundaries to what we can expect algorithms to accomplish, and it is important to be able to recognize this distinction to better focus attention to problems that can be solved. This has real implications in software engineering, especially in testing and verification, where developers can identify the kinds of problems that theory says can't be solved by a general algorithm. We looked at how formal methods and automated testing tools attempt to work around these issues, even if they aren’t able to eliminate them entirely. Overall, the report explores abstract theory and how it relates to practical tools and thinking patterns which are highly useful when building software. Understanding these concepts helps not only with improving algorithm efficiency, but also with understanding what kinds of problems to focus on in the first place. 


\section{Week by Week}\label{homework}

\subsection{Week 1}

% \subsubsection*{Notes}

% 1. Automata

% - machine model which has variety of computations without memory
% - traversal (trees, graphs, nodes)
% - Turing machine
% - traffic light

% - parking machine
% - mechanical calculators
% - streaming
% - regular expressions (regex): such as NLP, parsing, coding theory, file management

% automata: implement algorithms, admit algorithmic instructions.


% 2. Computability And complexity

% - universal language to analyze algorithms/problems/computations for:
% - efficiency/(runtime, spacetime, etc) resource/scaling
% - comparison between complexity, hardness of problems
% - complexity classes
% - big-o-notation
% - reducing problems
% - P vs NP, sat problems

% 3. Graph Algorithms

% - network theory ( data, mechanical, chemical, operations)

% 2/6

% 1. Automata theory

% states: positions: (head) $\overrightarrow{l}$  () $\overrightarrow{t}$  (end)

% ex.1 parking machine:
% charge/hr = 25c
% accepted: coins, no change, no 1, 2c coins

% states:
% - value paid so far (0)
% 0 -> [5,10,25]

% ex.2 valid var names:
% 1. index 0: letter?
% 2. index n > 0: letter or digit or symbol

\subsubsection*{Comments and Questions}

How do automata models like parking machines demonstrate the concepts of states and transitions? How does complexity theory contribute to these models?

\subsection{Week 2}

\subsubsection*{Notes}

Chapter 2.1 of ITALC discusses the basic model of computation that demonstrates the structure and components of finite automata. It explains that a finite automaton is a simple computational machine which is composed of a set of states, including a start state, and one or more end states. As the automaton reads an input, it transitions between states based on a set of rules, the transition functions. The section uses examples with simple intuition, such as an on/off switch, to show how a system can 'remember' essential information despite its simplicity. Also, it talks about the difference between deterministic and non-deterministic models. A deterministic model is one in which each state has a unique transition for each input, and a non-deterministic model is one where multiple transitions may be possible for one input. Although these two models are very different in practice, they recognize the same set of basic regular language. 

\subsubsection*{Homework 1}

\textbf{Introduction to Automata Theory:}

Homework: Characterize all the accepted words (i.e., describe exactly those words that get recognized).

Answer:  $[5, 5, 5, 5, 5], [5, 5, 5, 10], [5, 5, 10, 5], [5, 10, 5, 5], [5, 10, 10], [10, 5, 5, 5], [10, 5, 10], [10, 10, 5]$

Homework: Characterize all the accepted words. Can you describe them via a regular expression?

Answer: Any word which ends in 'pay' will result in end state being unlocked

\textbf{Deterministic and Non-Deterministic Finite Automata:}

Homework: Determine for the following words, if they are contained in $L_{1}$, $L_{2}$, or $L_{3}$.

Answer:
\begin{table}[!ht]
  \centering
  \begin{tabular}{||c c c||} 
   \hline
   word & A\_1 & A\_2 \\ [0.5ex] 
   \hline\hline
   aaa & no  & yes  \\ 
   aab & yes  & no  \\
   aba & no  & no  \\
   abb & no  & no  \\
   baa & no  & yes \\ 
   bab & no & no \\
   bba & no & no \\
   bbb & no & no \\ [1ex] 
   \hline
  \end{tabular}
  \end{table}

Homework: Consider the DFA from above: Consider the paths corresponding to the words $w_{1}$ = 0010, $w_{2}$ = 1101, $w_{3}$ = 1100.\newline
Answer: $w_{1}$ and $w_{2}$ both achieve the end state

\subsubsection*{Comments and Questions}
Because finite automata have limited memory, their ability to recognize more complex language patterns is restricted. Will the range of what is categorized as a finite automaton broaden over time? How could we modify automata to recognize more complex language?

\subsection{Week 3}

% \subsubsection*{Notes}

\subsubsection*{Homework 2}

\underline{Exercise 2, 1-3:}

The file $\mathbf{dfa.py}$ implements a class representing a deterministic finite automaton(DFA) The automaton is defined by a set of states, inputs, transition functions, an initial state, and accepting states, all of which are initialized in the constructor function. The repr method offers a formatted string representation of the DFA for easy inspection. The run method processes an input word by iterating through its characters and transitions between states according to the defined rules. If an undefined transition is encountered or an invalid symbol is detected, the word is rejected. Finally, the refuse method generates a complementary DFA by changing the set of accepting states to accept words that the original DFA rejects.
\newline
The file $\mathbf{dfa-ex01.py}$ implements two DFAs, A1 and A2, and runs each through dfa.py to test them repsectively.

\lstinputlisting[language=Octave]{../labs/py-automata/dfa.py}
\lstinputlisting[language=Octave]{../labs/py-automata/dfa_ex01.py}

\begin{figure}
\begin{center}
\includegraphics[scale=0.2]{attachments/LabExercise4.jpg}
\end{center}
\caption{Lab Exercsie 4}
\end{figure}

\begin{figure}
  \begin{center}
\includegraphics[scale=0.2]{attachments/LabExercise4_2.jpg}
\end{center}
\caption{ITALC Exercise 2.2.4}
\end{figure}

\subsubsection*{Comments and Questions}

I occasionally find myself struggling to recognize the pattern when first looking at a new DFA. Is there an insightful method or trick to recognizing these patterns more easily?

\subsection{Week 4}

% \subsubsection*{Notes}

\subsubsection*{Homework 3}

\underline{Homework 1}

1. The language of the automata A2 can be described as starting with an a and having an odd length. Any b's that may occur must do so from state 2. This can be described as a regular expression $a((a|b)a)*$.

2. The extended transition functions can be evaluated as follows:

A1:
\begin{itemize}
  \item $\delta_1(1, a) = 2$
  \item $\delta_1(2, b) = 4$
  \item $\delta_1(4, a) = 2$
  \item $\delta_1(2, a) = 3$
\end{itemize}

A2:
\begin{itemize}
  \item $\delta_2(1, a) = 2$
  \item $\delta_2(2, b) = 1$
  \item $\delta_2(1, b) = 3$
  \item $\delta_2(3, a) = 3$
\end{itemize}

\underline{Homework 2}

\begin{tikzpicture}[shorten >=1pt,node distance=2.5cm,on grid,auto]
  % Nodes
  \node[state,initial] (q0) {$(q_0,p_0)$};
  \node[state,accepting] (qa) [right=of q0] {$(q_a,p_1)$};
  \node[state,accepting] (qb) [below=of qa] {$(q_b,p_0)$};
  \node[state] (trap) [below=of q0] {$\bot$};
  % Transitions from (q0,p0)
  \path[->]
      (q0) edge node {a} (qa)
           edge node [swap] {b} (trap)
  % Transitions from (q_a,p_1)
      (qa) edge node {b} (qb)
           edge[bend left] node {a} (trap)
  % Transitions from (q_b,p_0)
      (qb) edge node {a} (qa)
           edge[bend right] node [swap] {b} (trap)
  % Trap self-loop
      (trap) edge [loop left] node {a,b} (trap);
\end{tikzpicture}

2. When constructing the product automaton, a word w is processed by both components simultaneously. The word is said to be accepted iff the computation ends in a state $(q,p)$ where q is in F(1) and p is in F(2). This means that w is accepted by both A(1) and A(2), so L(A) can be the intersection between the two. 

3. In the product automaton constrction, the set of states is always $Q_1 * Q_2$ and the transitions are the same. To find an automaton which is the union of the two languages, we change the accepting states. So, a word w is accepted by A' iff it is accepted by at least one of the original automata, which gives L(A') to be the union of L(A1) and L(A2). 

To summarize:

\[
\begin{aligned}
Q &= Q^{(1)}\times Q^{(2)}, \\
\delta((q,p),x) &= (\delta^{(1)}(q,x),\,\delta^{(2)}(p,x)), \\
q_0 &= (q_0^{(1)},q_0^{(2)}), \\
F' &= \Bigl\{(q,p) \in Q^{(1)}\times Q^{(2)} : q\in F^{(1)} \text{ or } p\in F^{(2)}\Bigr\}.
\end{aligned}
\]

\underline{Exercise 2.2.7 - ITALC}

\textbf{Theorem.} Let \(A = (Q,\Sigma,\delta,q_0,F)\) be a DFA and let \(q \in Q\) be such that 
\[
\delta(q,a) = q \quad \text{for all } a \in \Sigma.
\]
Then for all \(w \in \Sigma^*\), \(\hat{\delta}(q,w) = q\).

\textbf{Proof.} We prove by induction on the length of \(w\).

\textit{Base Case:} \(w = \varepsilon\) (the empty string).  
By definition of the extended transition function,
\[
\hat{\delta}(q,\varepsilon) = q.
\]

\textit{Inductive Step:} Assume that for some \(w \in \Sigma^*\), \(\hat{\delta}(q,w) = q\).  
Let \(a \in \Sigma\). Then,
\[
\hat{\delta}(q, wa) = \delta(\hat{\delta}(q, w), a) = \delta(q, a) = q.
\]
Thus, by the principle of induction, for all \(w \in \Sigma^*\), \(\hat{\delta}(q,w) = q\).


\subsubsection*{Comments and Questions}

Do automata with some form of union or intersection have real world applications relating to natural language processing? Could they be used to improve parsing in conversational AI? 

\subsection{Week 5}

\subsubsection*{Notes}
Chapter 3.1-3.2 of ITALC explores regular expressions as a powerful way to more accurately describe formal languages, specifically being useful in various text processing. Regular expressions are built using basic operations such as union, concatenation,and Kleene closure, which combines simple patterns in order to create more complex expressions. These operations and the way they are used is defined through standard set operations. Union is able to combine languages by joining their elements, concatenation makes new strings by combining parts from each language, and Kleene closure allows for the generation of all potential strings. Also, Kleene's algorithm is shown as a structured method in order to change DFAs into regular expressions, usually relying on induction that builds expressions incrementally that represent state transitions. With larger automata, this algorithm can become very computationally expensive. Finally, the transition between regular expressions and automata is bidirectional, meaning you can go from one to the other and back again. 

\subsubsection*{Homework 4}

Homework 1:

Let $\mathcal{A} = (Q, \Sigma, \delta: Q \times \Sigma \to Q, q_0, F)$ be a DFA.

\begin{enumerate}
    \item \textbf{Viewing $\mathcal{A}$ as an NFA:}

    A DFA is a specific case of an NFA where the transition function returns only one next state for each input symbol and state. To show the DFA as an NFA, we make a new transition function $\delta': Q \times \Sigma \to \mathcal{P}(Q)$ sucsoh that:
    \[
    \delta'(q, a) = \{ \delta(q, a) \}
    \]
    This shows that each deterministic transition becomes a singleton set in the NFA transition function. All other components of the DFA remain unchanged:
    \begin{itemize}
        \item $Q' = Q$
        \item $\Sigma$ remains the same
        \item $q_0' = q_0$
        \item $F' = F$
    \end{itemize}

    \item \textbf{General Construction:}

    Given a DFA $\mathcal{A} = (Q, \Sigma, \delta, q_0, F)$, we define an equivalent NFA $\mathcal{A}' = (Q', \Sigma, \delta', q_0', F')$ as follows:
    \begin{align*}
        Q' &= Q \\
        \Sigma' &= \Sigma \\
        q_0' &= q_0 \\
        F' &= F \\
        \delta'(q, a) &= \{ \delta(q, a) \} \quad \text{for all } q \in Q, a \in \Sigma
    \end{align*}

    \item \textbf{Justification:}

    This design ensures that $\mathcal{A}'$ accepts the same language as $\mathcal{A}$. Because each transition in the NFA $\delta'$ corresponds to the DFA's transition , the NFA has exactly one computational path for any given input string, mirroring the DFA. So,
    \[
    L(\mathcal{A}) = L(\mathcal{A}')
    \]
\end{enumerate}

Homework 2: 

\begin{enumerate}
    \item \textbf{Language accepted by $\mathcal{A}$:}  
    The NFA accepts all binary strings that have the substring \texttt{010}.

    \item \textbf{Specification of $\mathcal{A}$:}  
    \begin{itemize}
        \item $Q = \{q_0, q_1, q_2, q_3\}$
        \item $\Sigma = \{0, 1\}$
        \item $q_0$ is the start state
        \item $F = \{q_3\}$
        \item Transition function $\delta$:
        \begin{align*}
            \delta(q_0, 0) &= \{q_0\} \\
            \delta(q_0, 1) &= \{q_0, q_1\} \\
            \delta(q_1, 0) &= \{q_2\} \\
            \delta(q_2, 0) &= \{q_3\}, \quad \delta(q_2, 1) = \{q_1\} \\
            \delta(q_3, 0) &= \{q_3\}, \quad \delta(q_3, 1) = \{q_3\}
        \end{align*}
    \end{itemize}

    \item \textbf{Extended transition: compute $\hat{\delta}(q_0, 10110)$ step-by-step:}
    \begin{itemize}
        \item Step 0: $\{q_0\}$
        \item Read 1: $\{q_0, q_1\}$
        \item Read 0: $\{q_0, q_2\}$
        \item Read 1: $\{q_0, q_1\}$
        \item Read 1: $\{q_0, q_1\}$
        \item Read 0: $\{q_0, q_2\}$
        \item Final result: $\hat{\delta}(q_0, 10110) = \{q_0, q_2\}$
    \end{itemize}

    \item \textbf{All paths for $v = 1100$ and $w = 1010$:}

    \textbf{Step-by-step for $v = 1100$:}
    \begin{itemize}
        \item Start at $\{q_0\}$
        \item Read 1: $\{q_0, q_1\}$
        \item Read 1: $\{q_0, q_1\}$
        \item Read 0: $\{q_0, q_2\}$
        \item Read 0: $\{q_0, q_3\}$ → accepted
    \end{itemize}

    \textbf{Step-by-step for $w = 1010$:}
    \begin{itemize}
        \item Start at $\{q_0\}$
        \item Read 1: $\{q_0, q_1\}$
        \item Read 0: $\{q_0, q_2\}$
        \item Read 1: $\{q_0, q_1\}$
        \item Read 0: $\{q_0, q_2\}$ → rejected
    \end{itemize}

    \textbf{Common diagram of all paths:}

    \begin{center}
    \begin{tikzpicture}[->, node distance=2cm, on grid, auto]
      \node[state, initial] (q0) {$q_0$};
      \node[state] (q1) [right=of q0] {$q_1$};
      \node[state] (q2) [right=of q1] {$q_2$};
      \node[state, accepting] (q3) [right=of q2] {$q_3$};

      \path (q0) edge [loop above] node {0} (q0)
            (q0) edge [bend left] node {1} (q1)
            (q1) edge [bend left] node {0} (q2)
            (q2) edge [bend left] node {1} (q1)
            (q2) edge [bend left] node {0} (q3)
            (q3) edge [loop above] node {0,1} (q3);

      % Highlight sample path for v = 1100
      \draw[red, thick, ->] (q0) to[bend left=30] node {1} (q1);
      \draw[red, thick, ->] (q1) to[bend left=30] node {0} (q2);
      \draw[red, thick, ->] (q2) to[bend left=30] node {0} (q3);
    \end{tikzpicture}
    \end{center}

    \item \textbf{Powerset construction for DFA $\mathcal{A}^D$:}

    Start from state $\{q_0\}$:
    \begin{itemize}
        \item $\delta(\{q_0\}, 0) = \{q_0\}$
        \item $\delta(\{q_0\}, 1) = \{q_0, q_1\}$
    \end{itemize}
    Continue expanding:
    \begin{itemize}
        \item $\delta(\{q_0, q_1\}, 0) = \{q_0, q_2\}$
        \item $\delta(\{q_0, q_1\}, 1) = \{q_0, q_1\}$
        \item $\delta(\{q_0, q_2\}, 0) = \{q_0, q_3\}$
        \item $\delta(\{q_0, q_2\}, 1) = \{q_0, q_1\}$
        \item $\delta(\{q_0, q_3\}, 0) = \{q_0, q_3\}$, etc.
    \end{itemize}
    Accepting states: all subsets that contain $q_3$

    \item \textbf{Verification and minimization:}  
    $L(\mathcal{A}) = L(\mathcal{A}^D)$ by construction. A simple DFA for the language contains substring '010' has 4 states, so $\mathcal{A}^D$ can be simplified further.
\end{enumerate}

\subsubsection*{Comments and Questions}

How does the powerset construction in Homework 2 show the relationship between DFAs and NFAs, and how does this relate to the transformation approach in Homework 1?

\subsection{Week 6}

% \subsubsection*{Notes}

\subsubsection*{Comments and Questions}

Why does state equivalence require all the suffixes, which might be infinite? Is there a bound on suffix length past which no new differences between states can arise?

\subsection{Week 7}

% \subsubsection*{Notes}

\subsubsection*{Homework 5}

\textbf{DFA Regular Expressions and State Elimination (Exercise 3.2.1)}

Given a transition table:
\begin{center}
\begin{tabular}{|c|c|c|}
\hline
       & 0   & 1   \\ \hline
$\rightarrow q_1$ & $q_2$ & $q_1$ \\ \hline
$q_2$ & $q_3$ & $q_1$ \\ \hline
$*q_3$ & $q_3$ & $q_2$ \\ \hline
\end{tabular}
\end{center}

\medskip

\textbf{a. \(R_{ij}^{(0)}\) (Initial regular expressions with no intermediate states)}\\[1mm]
Let 
\[
R_{ij}^{(0)} = \begin{cases}
\text{the symbol(s) labeling direct transitions from state } i \text{ to state } j,\\[1mm]
\varnothing, \quad \text{if no direct transition,}\\[1mm]
\epsilon, \quad \text{if } i = j.
\end{cases}
\]
Thus,
\[
\begin{array}{rcl}
R_{11}^{(0)} &=& \epsilon, \quad R_{12}^{(0)} = 0, \quad R_{13}^{(0)} = \varnothing,\\[1mm]
R_{21}^{(0)} &=& 1, \quad R_{22}^{(0)} = \epsilon, \quad R_{23}^{(0)} = 0,\\[1mm]
R_{31}^{(0)} &=& \varnothing, \quad R_{32}^{(0)} = 1, \quad R_{33}^{(0)} = 0 \mid \epsilon.
\end{array}
\]

\medskip

\textbf{b. \(R_{ij}^{(1)}\) (Using state \(q_1\) as intermediate)}\\[1mm]
Using the formula
\[
R_{ij}^{(k)} = R_{ij}^{(k-1)} \cup R_{i k}^{(k-1)}\,(R_{kk}^{(k-1)})^*\, R_{kj}^{(k-1)},
\]
with \(q_1\) as the singular intermediate state we get:
\[
\begin{array}{rcl}
R_{11}^{(1)} &=& \epsilon \cup (\epsilon)(\epsilon)^*\epsilon = \epsilon,\\[1mm]
R_{12}^{(1)} &=& 0 \cup (\epsilon)(\epsilon)0 = 0,\\[1mm]
R_{13}^{(1)} &=& \varnothing \cup (\epsilon)(\epsilon)\varnothing = \varnothing,\\[1mm]
R_{21}^{(1)} &=& 1 \cup (1)(\epsilon)^*\epsilon = 1,\\[1mm]
R_{22}^{(1)} &=& \epsilon \cup (1)(\epsilon)0 = 10,\\[1mm]
R_{23}^{(1)} &=& 0 \cup (1)(\epsilon)\varnothing = 0,\\[1mm]
R_{31}^{(1)} &=& \varnothing \cup (1)(\epsilon)^*\epsilon = \varnothing,\\[1mm]
R_{32}^{(1)} &=& 1 \cup (1)(\epsilon)0 = 1,\\[1mm]
R_{33}^{(1)} &=& 0 \cup (1)(\epsilon)\varnothing = 0.
\end{array}
\]

\medskip

\textbf{c. \(R_{ij}^{(2)}\) (Using \(q_1\) and \(q_2\) as intermediates)}\\[1mm]
Now we add \(q_2\) as an intermediate state:
\[
\begin{array}{rcl}
R_{11}^{(2)} &=& \epsilon \cup 0\,(10)^*\,1,\\[1mm]
R_{12}^{(2)} &=& 0 \cup 0\,(10)^*\,10,\\[1mm]
R_{13}^{(2)} &=& 0\,(10)^*\,0,\\[1mm]
R_{21}^{(2)} &=& 1 \cup 10\,(10)^*\,1,\\[1mm]
R_{22}^{(2)} &=& 10\,(10)^*\,10 \cup \epsilon,\\[1mm]
R_{23}^{(2)} &=& 0 \cup 10\,(10)^*\,0,\\[1mm]
R_{31}^{(2)} &=& \varnothing \cup 1\,(10)^*\,1,\\[1mm]
R_{32}^{(2)} &=& 1 \cup 1\,(10)^*\,10,\\[1mm]
R_{33}^{(2)} &=& 0 \cup 1\,(10)^*\,0.
\end{array}
\]

\medskip

\textbf{d. Regular Expression for the Language}\\[1mm]
Start state: \(q_1\) \quad Final state: \(q_3\).\\[1mm]
The regex is \(R_{13}^{(2)}\), so
\[
R = 0\,(10)^*\,0.
\]

\medskip

\textbf{e. Transition Diagram and State Elimination (eliminate \(q_2\))}\\[1mm]
A simpler diagram of the DFA is:
\[
q_1 \xrightarrow{0} q_2 \xrightarrow{0} q_3\quad (\text{final})
\]
with loops and additional transitions that are indicated in the original DFA. After getting rid of \(q_2\), the path from \(q_1\) to \(q_3\) becomes:
\[
(1 \cup 01)00(0),
\]
so final regular expression becomes
\[
R = (1 \cup 01)00(0).
\]

\bigskip

\textbf{DFA Regular Expressions and State Elimination (Exercise 3.2.2)}

Given the transition table:
\begin{center}
\begin{tabular}{|c|c|c|}
\hline
       & 0   & 1   \\ \hline
$\rightarrow q_1$ & $q_2$ & $q_3$ \\ \hline
$q_2$ & $q_1$ & $q_3$ \\ \hline
$*q_3$ & $q_2$ & $q_1$ \\ \hline
\end{tabular}
\end{center}

\medskip

\textbf{a. \(R_{ij}^{(0)}\) (Initial regular expressions with no intermediate states)}\\[1mm]
Let
\[
\begin{array}{rcl}
R_{11}^{(0)} &=& \epsilon, \quad R_{12}^{(0)} = 0, \quad R_{13}^{(0)} = 1,\\[1mm]
R_{21}^{(0)} &=& 0, \quad R_{22}^{(0)} = \epsilon, \quad R_{23}^{(0)} = 1,\\[1mm]
R_{31}^{(0)} &=& 1, \quad R_{32}^{(0)} = 0, \quad R_{33}^{(0)} = \epsilon.
\end{array}
\]

\medskip

\textbf{b. \(R_{ij}^{(1)}\) (Using \(q_1\) as intermediate)}\\[1mm]
Apply:
\[
R_{ij}^{(1)} = R_{ij}^{(0)} \cup R_{i1}^{(0)}\,(R_{11}^{(0)})^*\,R_{1j}^{(0)}.
\]
This gives:
\[
\begin{array}{rcl}
R_{11}^{(1)} &=& \epsilon \cup \epsilon\cdot\epsilon\cdot\epsilon = \epsilon,\\[1mm]
R_{12}^{(1)} &=& 0 \cup \epsilon\cdot\epsilon\cdot 0 = 0,\\[1mm]
R_{13}^{(1)} &=& 1 \cup \epsilon\cdot\epsilon\cdot 1 = 1,\\[1mm]
R_{21}^{(1)} &=& 0 \cup 0\cdot\epsilon\cdot\epsilon = 0,\\[1mm]
R_{22}^{(1)} &=& \epsilon \cup 0\cdot\epsilon\cdot 0 = \epsilon \cup 00,\\[1mm]
R_{23}^{(1)} &=& 1 \cup 0\cdot\epsilon\cdot 1 = 1 \cup 01,\\[1mm]
R_{31}^{(1)} &=& 1 \cup 1\cdot\epsilon\cdot\epsilon = 1,\\[1mm]
R_{32}^{(1)} &=& 0 \cup 1\cdot\epsilon\cdot 0 = 0 \cup 10,\\[1mm]
R_{33}^{(1)} &=& \epsilon \cup 1\cdot\epsilon\cdot 1 = \epsilon \cup 11.
\end{array}
\]

\medskip

\textbf{c. \(R_{ij}^{(2)}\) (Using \(q_1\) and \(q_2\) as intermediates)}\\[1mm]
Apply:
\[
R_{ij}^{(2)} = R_{ij}^{(1)} \cup R_{i2}^{(1)}\,(R_{22}^{(1)})^*\,R_{2j}^{(1)}.
\]
For example:
\[
\begin{array}{rcl}
R_{11}^{(2)} &=& \epsilon \cup 0\,( (\epsilon \cup 00) )^*\;0,\\[1mm]
R_{12}^{(2)} &=& 0 \cup 0\,( (\epsilon \cup 00) )^*\;(\epsilon \cup 00),\\[1mm]
R_{13}^{(2)} &=& 1 \cup 0\,( (\epsilon \cup 00) )^*\;(1 \cup 01),
\end{array}
\]
and in a similar manner for the other entries.

\medskip

\textbf{d. Regular Expression for the Language}\\[1mm]
Start state: \(q_1\) \quad Final state: \(q_3\).\\[1mm]
Thus,
\[
R = R_{13}^{(2)} = 1 \cup 0\,( (\epsilon \cup 00) )^*(1 \cup 01).
\]

\medskip

\textbf{e. Transition Diagram and State Elimination (eliminate \(q_2\))}\\[1mm]
A simplified diagram is as follows:
\[
q_1 \xrightarrow{0} q_2 \xrightarrow{0} q_1 \quad \text{and} \quad q_3 \xleftarrow{1} q_3.
\]
By getting rid of \(q_2\) and considering the other paths:
\[
q_1 \xrightarrow{0} q_2 \xrightarrow{1} q_3 \quad \Rightarrow \quad 01,
\]
\[
q_1 \xrightarrow{0} q_2 \xrightarrow{0} q_1 \quad \Rightarrow \quad 00 \text{ (loop)},
\]
the final regex becomes:
\[
R = \left( 1 \cup 0(00)^*1 \right).
\]

\bigskip

\textbf{DFA Minimization (Exercise 4.4.1)}

Given the transition table:
\begin{center}
\begin{tabular}{|c|c|c|}
\hline
       & 0   & 1   \\ \hline
$\rightarrow A$ & $B$ & $A$ \\ \hline
$B$ & $A$ & $C$ \\ \hline
$C$ & $D$ & $B$ \\ \hline
$*D$ & $D$ & $A$ \\ \hline
$E$ & $D$ & $F$ \\ \hline
$F$ & $G$ & $E$ \\ \hline
$G$ & $F$ & $G$ \\ \hline
$H$ & $G$ & $D$ \\ \hline
\end{tabular}
\end{center}

\medskip

\textbf{a. Table of Distinguishabilities}

Final table (X = marked/inequivalent, blank = equivalent):
\begin{table}[h]
  \centering
  \begin{tabular}{c|ccccccc}
     & B & C & D & E & F & G & H \\ \hline
  A  & X & X & X & X & X &   & X \\
  B  &   & X & X & X &   & X & X \\
  C  &   &   & X &   & X & X & X \\
  D  &   &   &   & X & X & X & X \\
  E  &   &   &   &   & X & X & X \\
  F  &   &   &   &   &   & X & X \\
  G  &   &   &   &   &   &   & X \\
  H  &   &   &   &   &   &   &   \\
  \end{tabular}
  \caption{Table of distinguishable state pairs}
  \label{tab:distinguishables}
  \end{table}
The unmarked (equivalent) states are \(E\), \(F\), and \(G\).

\medskip

\textbf{b. Minimum-State Equivalent DFA}
Define the minimized DFA \(M'=(Q',\Sigma,\delta',q_0',F')\) as follows:

\[
\begin{aligned}
Q'    &= \{\, [AG],\,[BF],\,[CE],\,[D],\,[H] \,\},\\
\Sigma&= \{0,1\},\\
q_0'  &= [AG],\\
F'    &= \{[D]\}.
\end{aligned}
\]

\medskip

The transition function \(\delta'\) is given by:

\[
  \begin{array}{c|cc}
    \delta'    & 0           & 1         \\
    \hline
    \text{[AG]} & \text{[BF]} & \text{[AG]} \\
    \text{[BF]} & \text{[AG]} & \text{[CE]} \\
    \text{[CE]} & \text{[D]}  & \text{[BF]} \\
    \text{[D]}  & \text{[D]}  & \text{[AG]} \\
    \text{[H]}  & \text{[AG]} & \text{[D]}
  \end{array}
\]

\textbf{DFA Minimization (Exercise 4.4.2)}

Given the transition table:
\begin{center}
\begin{tabular}{|c|c|c|}
\hline
       & 0   & 1   \\ \hline
$\rightarrow A$ & \(B\) & \(E\) \\ \hline
\(B\) & \(C\) & \(F\) \\ \hline
\(*C\) & \(D\) & \(H\) \\ \hline
\(D\) & \(E\) & \(I\) \\ \hline
\(E\) & \(F\) & \(H\) \\ \hline
\(*F\) & \(G\) & \(B\) \\ \hline
\(G\) & \(H\) & \(B\) \\ \hline
\(H\) & \(I\) & \(C\) \\ \hline
\(*I\) & \(A\) & \(E\) \\ \hline
\end{tabular}
\end{center}

\medskip

\textbf{a. Table of Distinguishabilities}\\[1mm]
Final states: \(C\), \(F\), and \(I\).\\[1mm]
Mark all the pairs with one final and one non-final state, then make distinctions. The final table (X = distinguishable) is:

\begin{table}[h]
  \centering
  \begin{tabular}{c|cccccccc}
     & B & C & D & E & F & G & H & I\\ \hline
  A  & X & X & X & X & X & X & X & X\\
  B  &   & X & X & X & X & X & X & X\\
  C  &   &   & X & X & X & X & X & X\\
  D  &   &   &   & X & X & X & X & X\\
  E  &   &   &   &   & X & X & X & X\\
  F  &   &   &   &   &   & X & X & X\\
  G  &   &   &   &   &   &   & X & X\\
  H  &   &   &   &   &   &   &   & X\\
  \end{tabular}
  \caption{Table of distinguishable state pairs}
  \label{tab:distinguishables2}
  \end{table}

\medskip

\textbf{b. Minimum-State Equivalent DFA}

Define \(M = (Q,\Sigma,\delta,q_0,F)\) where
\[
\begin{aligned}
Q      &= \{\text{[A]},\text{[B]},\text{[C]},\text{[D]},\text{[E]},\text{[F]},\text{[G]},\text{[H]},\text{[I]}\},\\
\Sigma &= \{0,1\},\\
q_0    &= \text{[A]},\\
F      &= \{\text{[C]},\text{[F]},\text{[I]}\}.
\end{aligned}
\]
The transition function \(\delta\) is given by:

\[
\begin{array}{c|cc}
\delta      & 0           & 1           \\\hline
\text{[A]} & \text{[B]} & \text{[E]} \\
\text{[B]} & \text{[C]} & \text{[F]} \\
\text{[C]} & \text{[D]} & \text{[H]} \\
\text{[D]} & \text{[E]} & \text{[I]} \\
\text{[E]} & \text{[F]} & \text{[H]} \\
\text{[F]} & \text{[G]} & \text{[B]} \\
\text{[G]} & \text{[H]} & \text{[B]} \\
\text{[H]} & \text{[I]} & \text{[C]} \\
\text{[I]} & \text{[A]} & \text{[E]}
\end{array}
\]

\subsubsection*{Comments and Questions}

In Exercise 3.2.1, how does the order of state elimination change the final regular expression’s structure, and could a different order get us to a more simplified form?

\subsection{Week 8}

% \subsubsection*{Notes}

\subsubsection*{Comments and Questions}

Suppose you have a recursively enumerable language L accepted by a TM M. What conditions are needed to guarantee there is a halting decider M' for L, and how would you go about constructing such an M'?

\subsection{Week 9}

% \subsubsection*{Notes}
% Undecidability
% examples:
% 1. does str s contain substring t
% 2. is an integer a prime
% 3. is a real number x nonzero?
% 4. is a point x reachable from a point y
% 5. does a program p halt on some input w

% all these problems are about different objects within a turing machine as the universal computation model,
% we say a problem is decidable if it is decidable by a turing machine (algorithm)
% want to translate these to acceptance problems for a turing machine, problems of the form 'is W in the language of some turing machine'. this is what computability theory is about

% Halting Problem:
% Lh := { (M,W) | M TM (code), W in {0,1}, M accepts W} this is the language in set of pairs of binary strings
% enough to reduce to a binary alphabet
% can view every {0,1}* as a TM
% In turn, we can enumerate all the words in {0,1}* by NN

\subsubsection*{Homework 6}

\noindent\textbf{Exercise A (1).} TM for \(L=\{1\,0^n : n\in\mathbb{N}\}\) that outputs \(1\,0^{n+1}\):
\[
M_1 = \bigl(Q_1,\Sigma,\Gamma,\delta_1,q_0,B,F_1\bigr),
\quad
Q_1=\{q_0,q_1,q_{\mathrm{acc}},q_{\mathrm{rej}}\},
\quad
\Sigma=\{0,1\},\;\Gamma=\{0,1,B\},\;
F_1=\{q_{\mathrm{acc}}\}.
\]
\[
\delta_1\colon Q_1\times\Gamma\to\Gamma\times\{L,R,S\}\times Q_1
\]
\[
\begin{array}{rl}
\delta_1(q_0,1)=(1,R,q_1), & \delta_1(q_0,0)=(\_,\_,q_{\mathrm{rej}}),\\
\delta_1(q_0,B)=(\_,\_,q_{\mathrm{rej}}), &
\delta_1(q_1,0)=(0,R,q_1),\\
\delta_1(q_1,B)=(0,S,q_{\mathrm{acc}}).
\end{array}
\]

\medskip
\noindent\textbf{Exercise A (2).} TM for \(L=\{1\,0^n\}\) that outputs just “1”:
\[
M_2 = \bigl(Q_2,\Sigma,\Gamma,\delta_2,q_0,B,F_2\bigr),
\quad
Q_2=\{q_0,q_1,q_2,q_{\mathrm{acc}},q_{\mathrm{rej}}\},
\quad
\Sigma=\{0,1\},\;\Gamma=\{0,1,B\},\;
F_2=\{q_{\mathrm{acc}}\}.
\]
\[
\delta_2\colon Q_2\times\Gamma\to\Gamma\times\{L,R,S\}\times Q_2
\]
\[
\begin{array}{rl}
\delta_2(q_0,1)=(1,R,q_1), & \delta_2(q_0,0)=(\_,\_,q_{\mathrm{rej}}),\\
\delta_2(q_0,B)=(\_,\_,q_{\mathrm{rej}}), &
\delta_2(q_1,0)=(0,R,q_1),\\
\delta_2(q_1,B)=(B,L,q_2), &
\delta_2(q_2,0)=(B,L,q_2),\\
\delta_2(q_2,1)=(1,S,q_{\mathrm{acc}}), &
\delta_2(q_2,B)=(\_,\_,q_{\mathrm{rej}}).
\end{array}
\]

\medskip
\noindent\textbf{Exercise A (3).} TM for all binary strings that swaps \(0\leftrightarrow1\):
\[
M_3 = \bigl(Q_3,\Sigma,\Gamma,\delta_3,q_0,B,F_3\bigr),
\quad
Q_3=\{q_0,q_{\mathrm{acc}}\},
\quad
\Sigma=\{0,1\},\;\Gamma=\{0,1,B\},\;
F_3=\{q_{\mathrm{acc}}\}.
\]
\[
\delta_3\colon Q_3\times\Gamma\to\Gamma\times\{L,R,S\}\times Q_3
\]
\[
\begin{array}{rl}
\delta_3(q_0,0)=(1,R,q_0), & \delta_3(q_0,1)=(0,R,q_0),\\
\delta_3(q_0,B)=(B,S,q_{\mathrm{acc}}).
\end{array}
\]

\subsubsection*{Homework 7}

\textbf{Exercise 1.1}\\
Let 
\[
L_1 = \{M \mid M\text{ halts on }\langle M\rangle\}.
\]
\(L_1\) is recursively enumerable but not decidable, and its complement is not recursively enumerable.

\textbf{Exercise 1.2}\\
Let 
\[
L_2 = \{(M,w) \mid M\text{ halts on }w\}.
\]
\(L_2\) is recursively enumerable but not decidable, and its complement is not recursively enumerable.

\textbf{Exercise 1.3}\\
Let 
\[
L_3 = \{(M,w,k)\mid M\text{ halts on }w\text{ within }k\text{ steps}\}.
\]
\(L_3\) is decidable (hence both recursively enumerable and co‑recursively enumerable).

\textbf{Exercise 2.1.} True. Argument: If \(M_1\) and \(M_2\) decide \(L_1\) and \(L_2\), then when you have an input \(w\) run \(M_1(w)\); if it accepts, accept; otherwise run \(M_2(w)\); if it accepts, accept; else it rejects. This always halts and decides if \(L_1\cup L_2\).

\textbf{Exercise 2.2.} True. Argument: If \(M\) decides \(L\), then when you have an input \(w\) run \(M(w)\) and flip the output (accept\(\leftrightarrow\)reject). The resulting machine will halt on every input and decides \(\overline L\).

\textbf{Exercise 2.3.} True. Argument: To decide \(L^*\), on an input \(w\) use dynamic programming principles. Set \(\mathrm{dp}[0]=\text{true}\) and for \(1\le i\le|w|\), let:
\[
\mathrm{dp}[i] = \exists\,j<i\;\bigl[\mathrm{dp}[j]\land M\bigl(w_{j+1}\dots w_i\bigr)\text{ accepts}\bigr].
\]
Then accept iff \(\mathrm{dp}[|w|]\). This halts in a finite amount of steps.

\textbf{Exercise 2.4.} True. Argument: If \(M_1\) and \(M_2\) recognize \(L_1\) and \(L_2\), combine their simulations on input \(w\). If either accepts, accept. This kind of decides \(L_1\cup L_2\), so \(L_1\cup L_2\) is r.e.

\textbf{Exercise 2.5.} False. Counterexample: The halting set 
\[
K=\{\langle M,w\rangle\mid M\text{ halts on }w\}
\]
is r.e.\, but its complement \(\overline K\) is not r.e. So, r.e.\ languages are not closed under the complement.

\textbf{Exercise 2.6.} True. Argument: To recognize \(L^*\), we nondeterministically split \(w=w_1\cdots w_k\), 'dovetail'-run the recognizer \(M\) on each \(w_i\), and accept if all accept. Every \(w\in L^*\) will have some accepting 'decomposition', so this semi‑decides \(L^*\), but not completely.


\subsubsection*{Comments and Questions}

1. What modifications are required to combine the “increment‐zeros” TM (Ex. 1) and the “bit‐swap” TM (Ex. 3) into a single 2‑tape TM that performs both operations in one pass?
2. Is there a way we can augment the standard TM model that would greatly enlarge the overall class of decidable languages?

\subsection{Week 10}

\subsubsection*{Notes}

input len n, interested in num of steps for n
also interested in maximal number of stepss
T: N -> R >= 0
looking at non negative functions from f,g:N -> R >= 0
we say the f is an element of O(g) iff f(n) <= M * g(n), we want this to hold true for some M > 0, and all n > N
this is a measure with which we can compare the growth of functions

\subsubsection*{Homework 8 and 9}
Due April 27th

\textbf{Exercise 1 (Warmup)}

\begin{align*}
f.\quad &\log\log n,\\
c.\quad &\log n,\\
b.\quad &e^{\log n} = n,\\
e.\quad &e^{2\log n} = n^2,\\
g.\quad &2^n,\\
d.\quad &e^n,\\
h.\quad &n!,\\
a.\quad &2^{2^n}.
\end{align*}

\textbf{Exercise 2}

\textbf{Exercise 2.1}  
\emph{Claim:} $f\in O(f)$.  
\noindent\emph{Proof.}  
Choose $c=1$ and any $n_0$. Then for all $n\ge n_0$,
\[
  f(n)\le 1\cdot f(n).
\]

\textbf{Exercise 2.2}  
\emph{Claim:} If $c>0$ is constant then $O(c\cdot f)=O(f)$.  
\noindent\emph{Proof.}  
\begin{itemize}
  \item[$\subseteq$] If $g\in O(c\,f)$ then there exist constants $C,n_1$ such that for all $n\ge n_1$,
  \[
    g(n)\le C\,(c\,f(n))=(C\,c)\,f(n),
  \]
  so $g\in O(f)$.
  \item[$\supseteq$] If $g\in O(f)$ then there exist constants $D,n_2$ such that for all $n\ge n_2$,
  \[
    g(n)\le D\,f(n)=(D/c)\,(c\,f(n)),
  \]
  so $g\in O(c\,f)$.
\end{itemize}

\textbf{Exercise 2.3}  
\emph{Claim:} If $f(n)\le g(n)$ for all $n\ge n_0$, then $O(f)\subseteq O(g)$.  
\noindent\emph{Proof.}  
Let $h\in O(f)$, so there exist $C,n_1$ such that for all $n\ge n_1$,
\[
  h(n)\le C\,f(n)\le C\,g(n).
\]
Thus $h\in O(g)$.

\textbf{Exercise 2.4}  
\emph{Claim:} If $O(f)\subseteq O(g)$ then $O(f+h)\subseteq O(g+h)$.  
\noindent\emph{Proof.}  
From $O(f)\subseteq O(g)$ we have $f\in O(g)$, so there exist $A,N$ such that for all $n\ge N$,
\[
  f(n)\le A\,g(n).
\]
If $k\in O(f+h)$ then there exist $C,n_0$ such that for all $n\ge n_0$,
\[
  k(n)\le C\bigl(f(n)+h(n)\bigr)
  \le C\bigl(A\,g(n)+h(n)\bigr)
  \le C\max(A,1)\,\bigl(g(n)+h(n)\bigr),
\]
hence $k\in O(g+h)$.

\textbf{Exercise 2.5}  
\emph{Claim:} If $h(n)>0$ for all $n$ and $O(f)\subseteq O(g)$ then $O(f\cdot h)\subseteq O(g\cdot h)$.  
\noindent\emph{Proof.}  
Again $f\in O(g)$, so there exist $A,N$ such that for all $n\ge N$,
\[
  f(n)\le A\,g(n).
\]
If $k\in O(f\cdot h)$ then there exist $C,n_0$ such that for all $n\ge n_0$,
\[
  k(n)\le C\,f(n)\,h(n)
  \le C\,(A\,g(n))\,h(n)
  =(C\,A)\,\bigl(g(n)\,h(n)\bigr),
\]
hence $k\in O(g\cdot h)$.

\textbf{Exercise 3}

\textbf{Exercise 3.1}  
\emph{Claim:} If $j\le k$ then $O(n^j)\subseteq O(n^k)$.  
\noindent\emph{Proof.}  
For all $n\ge1$, $n^j\le n^k$. If $f\in O(n^j)$ then there exist $C,n_0$ such that for all $n\ge n_0$,
\[
  f(n)\;\le\;C\,n^j\;\le\;C\,n^k,
\]
so $f\in O(n^k)$.

\textbf{Exercise 3.2}  
\emph{Claim:} If $j\le k$ then $O(n^j + n^k)\subseteq O(n^k)$.  
\noindent\emph{Proof.}  
For all $n\ge1$, $n^j + n^k \le 2\,n^k$. If $g\in O(n^j + n^k)$ then there exist $C,n_0$ such that for all $n\ge n_0$,
\[
  g(n)\;\le\;C\,(n^j + n^k)\;\le\;2C\,n^k,
\]
so $g\in O(n^k)$.

\textbf{Exercise 3.3}  
\emph{Claim:} $O\bigl(\sum_{i=0}^k a_i\,n^i\bigr)=O(n^k)$.  
\noindent\emph{Proof.}  
Let $A=\sum_{i=0}^k|a_i|$. For all $n\ge1$,
\[
  \sum_{i=0}^k a_i\,n^i
  \;\le\;
  \sum_{i=0}^k |a_i|\,n^k
  \;=\;
  A\,n^k.
\]
Hence any $h\in O\bigl(\sum a_i\,n^i\bigr)$ satisfies $h(n)\le C\,A\,n^k$ for some $C,n_0$, so $h\in O(n^k)$.

\textbf{Exercise 3.4}  
\emph{Claim:} $O(\log n)\subseteq O(n)$.  
\noindent\emph{Proof.}  
For all $n\ge2$, $\log n\le n$. If $p\in O(\log n)$ then there exist $C,n_0$ such that for all $n\ge\max(2,n_0)$,
\[
  p(n)\;\le\;C\,\log n\;\le\;C\,n,
\]
so $p\in O(n)$.

\textbf{Exercise 3.5}  
\emph{Claim:} $O(n\log n)\subseteq O(n^2)$.  
\noindent\emph{Proof.}  
For all $n\ge2$, $\log n\le n$, hence $n\log n\le n^2$. If $q\in O(n\log n)$ then there exist $C,n_0$ such that for all $n\ge\max(2,n_0)$,
\[
  q(n)\;\le\;C\,n\log n\;\le\;C\,n^2,
\]
so $q\in O(n^2)$.


\subsubsection*{Comments and Questions}

Why does Big-O ignore constant factors and smaller terms, and how does that help us choose and improve algorithms?

\subsection{Week 11/12}

% \subsubsection*{Notes}

\subsubsection*{Homework 10/11}

\textbf{Exercise 1}

\begin{enumerate}
  \item 
    \[
      \varphi_{1} \;=\;\neg\bigl((a\wedge b)\lor(\neg c\wedge d)\bigr)
    \]
    \begin{align*}
      &=\;\neg(a\wedge b)\;\land\;\neg(\neg c\wedge d) 
        &&\text{(De Morgan)}\\
      &=\;(\neg a\lor\neg b)\;\land\;(\neg\neg c\lor\neg d)
        &&\text{(De Morgan)}\\
      &=\;(\neg a\lor\neg b)\;\land\;(c\lor\neg d)
    \end{align*}

  \item 
    \[
      \varphi_{2} \;=\;\neg\bigl((p\lor q)\to(r\wedge\neg s)\bigr)
    \]
    \begin{align*}
      &=\;\neg\bigl(\neg(p\lor q)\;\lor\;(r\wedge\neg s)\bigr)
        &&\text{(implication }A\to B\equiv\neg A\lor B)\\
      &=\;\neg\neg(p\lor q)\;\land\;\neg(r\wedge\neg s)
        &&\text{(De Morgan)}\\
      &=\;(p\lor q)\;\land\;(\neg r\lor s)
    \end{align*}
\end{enumerate}

\textbf{Exercise 2}

\begin{align*}
\psi_1 &= (a\lor\neg b)\land(\neg a\lor b)\land(\neg a\lor\neg b) \\
       &\text{Try }a=0:\;(a\lor\neg b)\Rightarrow b=0,\;(\neg a\lor b)=1 
         \;\Longrightarrow\;(a,b)=(0,0)\text{ satisfies all.}
\\[1ex]
\psi_2 &= (\neg p\lor q)\land(\neg q\lor r)\land\neg(\neg p\lor r) \\
       &\equiv(\neg p\lor q)\land(\neg q\lor r)\land(p\land\neg r) \\
       &\quad\Rightarrow q=1,\;(\neg q\lor r)=0
         \;\Longrightarrow\;\text{unsatisfiable.}
\\[1ex]
\psi_3 &= (x\lor y)\land(\neg x\lor y)\land(x\lor\neg y)\land(\neg x\lor\neg y) \\
       &\implies x\iff y \\
       &\quad\{x,y\}=\{0,0\}\text{ fails }(x\lor y),\;
         \{1,1\}\text{ fails }(\neg x\lor\neg y)
         \;\Longrightarrow\;\text{unsatisfiable.}
\end{align*}

\textbf{Exercise 3 Sudoku}

Let
\[
x_{r,c,v} \;=\;
\begin{cases}
\text{true,} &\text{if the cell in row }r\text{ and column }c\text{ holds the digit }v,\\
\text{false,} &\text{otherwise},
\end{cases}
\]
for \(r,c,v\in\{1,\dots,9\}\).  Our overall formula is
\[
\Phi \;=\; C_{1}\;\land\;C_{2}\;\land\;C_{3}\;\land\;C_{4}\;\land\;C_{5}\;\land\;C_{6}\,.
\]

\subsubsection*{Constraint 1: Each cell has at least one digit}
\[
C_{1}\;=\;
\bigwedge_{r=1}^{9}\;\bigwedge_{c=1}^{9}
\Bigl(\,\bigvee_{v=1}^{9} x_{r,c,v}\Bigr)
\]

\subsubsection*{Constraint 2: No cell contains two different digits}
\[
C_{2}\;=\;
\bigwedge_{r=1}^{9}\;\bigwedge_{c=1}^{9}\;
\bigwedge_{1\le v<v'\le9}
\bigl(\,\neg x_{r,c,v}\;\lor\;\neg x_{r,c,v'}\bigr)
\]

\subsubsection*{Constraint 3: Each digit appears in every row}
\[
C_{3}\;=\;
\bigwedge_{r=1}^{9}\;\bigwedge_{v=1}^{9}
\Bigl(\,\bigvee_{c=1}^{9} x_{r,c,v}\Bigr)
\]

\subsubsection*{Constraint 4: Each digit appears in every column}
\[
C_{4}\;=\;
\bigwedge_{c=1}^{9}\;\bigwedge_{v=1}^{9}
\Bigl(\,\bigvee_{r=1}^{9} x_{r,c,v}\Bigr)
\]

\subsubsection*{Constraint 5: Each digit appears in each \(3\times3\) block}
Index blocks by \(b_r,b_c\in\{0,1,2\}\).  So
\[
C_{5}\;=\;
\bigwedge_{v=1}^{9}\;\bigwedge_{b_r=0}^{2}\;\bigwedge_{b_c=0}^{2}
\Bigl(\,\bigvee_{r=3b_r+1}^{3b_r+3}\;\bigvee_{c=3b_c+1}^{3b_c+3}\;
x_{r,c,v}\Bigr)
\]

\subsubsection*{Constraint 6: Respect the given clues}
The preset entries will be
\(\;G=\{(r_i,c_i,v_i)\mid i=1,\dots,m\}.\)
So
\[
C_{6}\;=\;
\bigwedge_{(r_i,c_i,v_i)\in G} x_{r_i,c_i,v_i}\,.
\]

\subsection*{Comments and Questions}

Is this the most efficient way to encode a problem like Sudoku? Are there other algorithmic approaches that are better or worse to solving such a problem?

\subsection{Week 13/14}

\subsubsection*{Homework 12/13}

\textbf{Exercise 1}

\textbf{1.} Applying Ford--Fulkerson algorithm:
\[
\begin{aligned}
&\text{Augment }s\to a\to d\to t\text{ by }8,\\
&\text{Augment }s\to b\to d\to t\text{ by }2,\\
&\text{Augment }s\to a\to c\to t\text{ by }2,\\
&\text{Augment }s\to b\to d\to c\to t\text{ by }6,\\
&\text{Augment }s\to b\to d\to a\to c\to t\text{ by }1.
\end{aligned}
\]
Resulting edge flows:
\[
f(s,a)=10,\quad f(s,b)=9,\quad f(a,c)=3,\quad f(a,d)=7,\quad f(b,d)=9,\quad f(d,c)=6,\quad f(c,t)=9,\quad f(d,t)=10.
\]
So, total flow is
\[
|f| \;=\; f(s,a)+f(s,b) \;=\; 10 + 9 \;=\; 19.
\]

\textbf{2.} In the final residual graph, the set of vertices reachable from $s$ is
\[
S=\{s,b\},\quad T=\{a,c,d,t\}.
\]
The capacity of the cut $(S,T)$ is
\[
\mathrm{cap}(s\to a)\;+\;\mathrm{cap}(b\to d)
\;=\;10 + 9 = 19,
\]
so, its a minimum cut.

\textbf{3.} A maximum flow doesn't need to be unique. Flow can be rerouted along directed cycles in the residual graph to get different edge‐flow assignments with same total value.

\textbf{Exercise 2}

\textbf{1.} The algorithm computes
\[
r \;=\;\sum_{k=1}^{n-1}\;\sum_{l=k+1}^{n}\;\sum_{m=1}^{l}1
\;=\;\sum_{k=1}^{n-1}\;\sum_{l=k+1}^{n}l
\;=\;\sum_{l=2}^{n}l\,(l-1)
\;=\;\sum_{l=1}^{n}(l^2 - l)
\;=\;\sum_{l=1}^{n}l^2 \;-\;\sum_{l=1}^{n}l.
\]
Using the identities
\[
\sum_{l=1}^n l \;=\;\frac{n(n+1)}2,
\qquad
\sum_{l=1}^n l^2 \;=\;\frac{n(n+1)(2n+1)}6,
\]
we obtain
\[
r
=\frac{n(n+1)(2n+1)}6 \;-\;\frac{n(n+1)}2
=\frac{n(n+1)\bigl[(2n+1)-3\bigr]}6
=\frac{n(n+1)\,2(n-1)}6
=\frac{n(n+1)(n-1)}3
=\frac{n^3-n}{3}.
\]

\textbf{2.} The total number of constant‐time increments is
\[
\sum_{k=1}^{n-1}\sum_{l=k+1}^{n}\sum_{m=1}^{l}1
=\Theta(n^3),
\]
so the worst‐case running time is
\[
O(n^3).
\]

\subsubsection*{Comments and Questions}

How do different augmenting paths change the residual graph, and can they have multiple minimum cuts with the same capacity?

\newpage
\section{Synthesis - Turing Machines and the Halting Problem}

In computational theory, one very important model is the concept of a Turing Machine [ALG] [ITALC]. These models are very abstract in nature, and are capable of performing a massive variety of tasks through the manipulation of different symbols on a ‘tape’ according to a certain set of rules, both of which can vary based on the machine’s application. This machine was initially introduced in 1936 by Alan Turing, who is prominently described as the grandfather of computing [ITALC]. Because of their large array of applications, Turing machines have become the primary framework for understanding the computation of algorithms, and have also become central to theoretical computer science across a variety of disciplines. These machines represent the idea of computability, and are able to define the boundaries of what complex problems are solvable by an algorithm, and which ones are simply beyond being solved by an algorithm [ITALC]. 

One such problem, the Halting Problem, which is largely associated with Turing machines, seeks to determine through an algorithm whether or not it is possible that a given Turing machine will halt when given a specific input, or if it will continue to run indefinitely until shut down by an outside source [ITALC]. Through a technique called diagonalization, Alan Turing was able to prove that the Halting Problem is undecidable, which means that there is no general algorithm that is capable of determining whether this problem can be solved for all possible machine-input pairs. This is a very important discovery in the field of theoretical computation - The fact that it is undecidable indicates that there is a major limitation in the theory of computation, and demonstrates that while many problems can be solved through an algorithm and classified as decidable, there are many problem still that are impossible to solve via an algorithmic approach. 

The fact that the Halting Problem is undecidable carries quite significant implications in the real world, particularly in software engineering debugging and testing processes. Because there isn’t a universal way to determine through an algorithm whether or not a piece of software will halt or run forever, software engineers have to rely on other techniques to make these determinations on their own. Some of the methods that are utilized to test software and decidability include heuristics, approximations, and various unit tests that are designed specifically for that piece of software [ITALC]. Unfortunately, these limitations lead to unreliability and the software may only be as good as its test, which is purely reliant on the skill and experience of those designing the testing methods. This calls for very careful design strategies, thorough testing methods, and being able to apply more formal verification methods when possible in order to proactively identify and solve potential problems that may arise with undecidable software. For example, in industry, automated testing frameworks are very common practice, which are able to periodically use algorithms and different code analysis tools to identify problematic code behavior which may not be apparent to those designing the software. These tools are the most modern and sophisticated way of ensuring that software is up to standards and will not run into the challenges of undecidability, but they still are not 100% guaranteed to be correct, or to guarantee termination.

Turing machines and the Halting Problem have not only pushed computing past previous theoretical limits, but also have real world implications upon both theoretical and practical applications of computing. This exploration has guided computer scientists to better understand the differences between solving problems efficiently, and those problems that are massively computationally intensive, or undecidable. Additionally, Alan Turing’s advancements have further encouraged the development of new advanced capabilities, particularly in areas such as artificial intelligence and machine learning, where undecidability helps guide the bounds of what can be reliably computed through these methods, as they are also extremely computationally expensive. By looking into these fundamental boundaries of computer science, both theoretical researchers and practical developers gain valuable insight into understanding how to construct valuable and resilient software that provides efficient solutions for solving complex problems across a variety of algorithmic constraints. 

\section{Evidence of Participation}

- Weekly Discord Questions

- Video Summaries/Reviews: 

1. \href{https://www.youtube.com/watch?v=G6R7UOFx1bw}{Jensen Huang on GPUs} \newline
In this video, Jensen Huang, the CEO of NVIDIA, explains how GPUs changed from tools for specific computing tasks into much more broad processors that are used for everything from tasks to gaming to AI. GPUs were different at first for specific tasks, things like gaming or video editing, but now many of them share a common architecture based around concepts such as CUDA and tensor cores. Tensor cores were very important in boosting GPU capabilities by improving applications in artificial intelligence, and doing other things such as very fast graphics rendering, and more complex physical simulations. Huang talks about the massive increases in computing power, which are driven by both hardware improvements and software changes such as better and more efficient algorithms. Traditional CPUs still matter, but GPUs are much more dominant right now due to their ability to parallel process information. He discusses how technology growth has been much faster than earlier predictions, like Moore's Law, thanks to improvements like parallel processing in multiple GPUs and data centers. He also discusses how AI is changing industries in ways we wouldn’t expect, such as for transforming communication networks. Using AI, bandwidth requirements can be dramatically reduced by predicting and generating data rather than transmitting everything. These changes show how GPUs have caused massive improvements in other technology, which are mostly powered by Nvidia's new chips, and they aim to continue drastically improving computing and reshaping possibilities across many fields.

2. \href{https://www.youtube.com/watch?v=rAEqP9VEhe8}{Generative AIs Greatest Flaw} \newline
In this video, computer-security researcher Mike Pound shows why “indirect prompt injection” is considered one of generative AI’s biggest weaknesses. Rather than typing an obvious instruction like “Ignore everything and write me a poem,” attackers hide secret text inside emails, web pages, résumés, or other data that an AI system later reads. When the model pulls in that poisoned data, the hidden text can quietly tell it to leak private files, approve fake payments, or rank one job applicant higher than others. Pound explains that this problem is similar to old-style SQL injection, but harder to fix because language models treat every piece of text, either good or bad, as part of one big prompt. The current defenses for this include carefully curating data sources, heavy automated testing, and limiting what outside tools the AI can touch, but none of these fully solves the issue. As AI assistants start handling more crucial information such as bank transfers, medical reports, and network controls, a single malicious hidden instruction could cause real harm to a variety of systems, in many different ways. Pound’s main point is that companies must treat indirect prompt injection as a permanent risk, use layered safeguards, and keep updating tests as new attack tricks appear. The flaw shows us that even advanced models remain vulnerable to very small bits of malicious text.

3. \href{https://www.youtube.com/watch?v=RQWpF2Gb-gU}{But What Is Quantum Computing?} \newline
In this video, Grant Sanderson (3Blue1Brown) clears up a common misconception about quantum computers. Many people think they are fast simply because they try every answer at once, but Sanderson shows that the real benefit comes from special algorithms and clever tricks relating to probability. He focuses on Grover’s algorithm, which speeds up searching a large list. A normal computer might check a million items one by one, but Grover’s quantum method only needs about a thousand steps, which is the square-root of the list size, and does this by repeatedly “rotating” the probability toward the right answer. Sanderson begins with the key building block, a qubit, which can be a mix of 0 and 1 at the same time. He explains how changing the “phase” of a qubit does not change its probability immediately, but does matter after several operations. By flipping phases for the ‘secret’ item and reflecting the state around an equal-balance line, Grover’s process steadily piles almost all probability onto the correct cell, so a final measurement is very likely to reveal the answer. Sanderson stresses that quantum gains are problem-specific: some tasks (like factoring) enjoy dramatic speed-ups, while most see smaller benefits. The takeaway is that quantum power comes from geometry and carefully designed rotations, not magic parallelism.

4. \href{https://www.youtube.com/watch?v=528Jc3q86F8}{Regular Expressions} \newline
In this discussion of regular expressions, Professor Brian Kernighan (and the Computerphile team) traces how regular expressions, created by mathematician Stephen Kleene in the 1950s, turned large state-machine diagrams into compact text patterns. A finite automaton that accepts strings like “abbb…c” can be drawn with circles and arrows, but writing the same rule as ab*c is far shorter and easier for programs to process. The star* means “zero or more,” and similar shorthand symbols cover optional parts, alternation, and groups. He shows that regular expressions, state-machine diagrams, and ‘Chomsky’ grammars are mathematically equivalent, that they match exactly the same strings, but regexes still win because of convenience. He also explains the problem of non-determinism, where a pattern engine might have several ways to match the next character. Computer scientists Michael Rabin and Dana Scott proved any non-deterministic automaton can be converted into a deterministic one, although the conversion can explode in size. Unix pioneer Ken Thompson tackled this by compiling small, fast pieces of assembly on the fly, while large tools like lex sometimes spend minutes converting big grammars up front. The overall idea is that regexes give programmers a powerful, concise language for finding text patterns, but efficient engines still rely on deep theoretical results and clever practical hacks.

5. \href{https://www.youtube.com/watch?v=bxpc9Pp5pZM}{Parsing Explained} \newline
In this video about parsing, Professor David Brailsford demonstrates parsing by inventing a tiny English-like language featuring robots, cats, dogs, and “two furry dice.” Parsing means breaking a sentence into parts such as the subject, verb, object, by using a formal grammar. He writes the grammar in something called Backus-Naur Form, which he explains is for the angle-bracket notation created for Algol 60(haven’t heard of this). He shows two strategies. In the first one, called top-down parsing, you start with the overall rule “sentence -> subject verb object,” push those goals on a stack, and match the words left to right. In the second, called bottom-up parsing, you begin with the words and merge them upward until the whole tree forms. Because his grammar lets “the robot” be parsed either as a shortcut noun phrase or as article + noun, the sentence “the robot stroked two furry dice” has two valid parse trees. That harmless ambiguity contrasts with math expressions such as “8 ÷ 4 ÷ 2,” where different trees give answers 1 or 4. Programming languages avoid such ambiguity by adding precedence and associativity rules, ensuring compilers always build the same tree. Brailsford ends by noting that clear grammars and reliable parsers are crucial for compilers, calculators, and any software that must understand structured input, from code to human language.



\newpage
\section{Conclusion}\label{conclusion}

Throughout this course, a variety of topics were discussed that provide a valuable and broad foundation for understanding the connections between theoretical computer science and practical software engineering. Some of the topics explored include deterministic and non deterministic finite automata, regular expressions, and formal languages, as well as more advanced topics such as Turing machines, complexity analysis, and decidability problems. These concepts have highlighted the critical importance of the theory underlying the modern day practices of software development and engineering.

Looking at the broader domain of software engineering, these foundational concepts aim to provide engineers with the essential ideas for understanding both the powerful capabilities and practical limitations of various different computing systems. For example, automata theory demonstrates how to design and develop efficient parsing algorithms, which are very important to practical design choices in domains such as compiler construction, text processing, and data validation tasks. Similarly, complexity analysis has a direct influence upon algorithm selection and optimization in problem solving, which helps to guide developers in making good decisions surrounding the design choices in scalable, performance-oriented software solutions in real world applications and industries. Additionally, Turing machines and the Halting problem provide invaluable insight into the absolute boundaries of what can be solved algorithmically, which helps engineers better understand what problems are theoretically impossible so that they can better focus their attention on problems with decidable, efficient, and feasible solutions.

Throughout the course materials, I found the topic of Turing machines and the Halting problem to be particularly interesting, provided that these concepts have laid the groundwork for modern day debugging and testing practices and solutions. Understanding that particular computing problems are simply undecidable - and why - has given me a new perspective on software debugging, and demonstrated that it is necessary to use a variety of approaches to solve, implement, debug, and test complex software problems. Being able to utilize the various approaches, such as heuristic algorithms, automated testing frameworks, and more formal verification methods has become increasingly important when I go about solving problems, and also allows for a much more streamlined approach. This understanding is particularly relevant in everyday software development, where reliability and efficiency are the most important outcomes in systems that affect virtually every industry worldwide, from finance, healthcare, and transportation. 

Reflecting on potential improvements for the course, I think more direct applications and code examples throughout the semester could be very valuable in student comprehension of challenging topics. Additionally, I believe that the course could benefit from more project based learning, seeing how these concepts can be applied through code to more real world problems would really bridge the gap between theory and practice. Overall, the course was very valuable, providing a theoretical framework to understanding why modern day coding practices and standards have been developed to the way they are today across a variety of industries that all utilize computing to some degree.


\begin{thebibliography}{99}
\bibitem[ALG]{alg} J. Weinberger, \href{https://github.com/jonweinb/algorithm-analysis-2025}{Algorithm Analysis} 2025.
\bibitem[ITALC]{italc} J.E. Hopcroft, {Introduction to Automata Theory, Language, and Computation}, Pearson, 2006.
\end{thebibliography}

\end{document}